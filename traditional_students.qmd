---
title: "traditional_students"
format: pdf
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: load-libraries
#| message: false
library(tidyverse)
library(viridis)
library(lme4)
library(glmnet)
library(pROC)
library(tidymodels)
library(rsample)
library(themis)
library(caret)
```

```{r}
#| label: load-data
df <- readRDS("data/model_df.rds")
df <- df |>
  mutate(
    studied_away = if_else(is.na(studied_away), FALSE, studied_away),
    prev_semGPA  = if_else(is.na(prev_semGPA), 4, prev_semGPA),
  )
```

```{r}
#| label: class-imbalance

# Imbalance of S/U vs A-F
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))

# Imbalance by student group
df |>
  group_by(student_group) |>
  summarise(
    n_total   = n(),
    n_true    = sum(is_SU == "1", na.rm = TRUE),
    n_false   = sum(is_SU == "0", na.rm = TRUE),
    prop_true = mean(is_SU == "1", na.rm = TRUE)
  ) |>
  arrange(desc(prop_true))

# Chose to drop data prior to Fall 2015
df <- df |>
  mutate(is_SU = factor(is_SU, levels = c(0, 1))) |>
  filter(term_enrolled > "Summer 2 2015")

# Slightly improved imbalance of S/U A-F
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))

```

```{r}
#| label: feature-imbalance

# Most courses are 1.0 credits
df |>
  count(max_units) |>
  mutate(prop = n / sum(n))

# A lot of uncommon term_units
df |>
  count(term_units) |>
  mutate(prop = n / sum(n)) |>
  print(n = Inf)

df <- df |>
  mutate(
    load_status = case_when(
      term_units < 4    ~ "underload",
      term_units == 4   ~ "normal",
      term_units > 4    ~ "overload",
      TRUE              ~ NA_character_   # handles any missing cases
    )
  )

# Relatively balanced when using categories instead of units
df |>
  count(load_status) |>
  mutate(prop = n / sum(n))
```

```{r}
#| label: train-test-split
set.seed(123)

split <- group_initial_split(df, group = masked_student_ID, prop = 0.8)
train_df <- training(split)
test_df  <- testing(split)

# want to predict on new students (no data leakage about students)
```

```{r}
#| label: preprocess
su_recipe_lasso <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

```

## LASSO 

```{r}
#| message: false
lasso_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),  # lambda
  mixture = 1        # pure LASSO
) |>
  set_engine("glmnet")

lasso_workflow <- workflow() |>
  add_model(lasso_spec) |>
  add_recipe(su_recipe_lasso)

folds <- group_vfold_cv(train_df, group = masked_student_ID, v = 5)
lambda_grid <- grid_regular(penalty(range = c(-4, 1)), levels = 20)
lasso_tuned <- tune_grid(
  lasso_workflow,
  resamples = folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, accuracy),
  control = control_grid(save_pred = TRUE, verbose = TRUE)
)

best_lambda <- select_best(lasso_tuned, metric = "roc_auc")

final_lasso <- finalize_workflow(lasso_workflow, best_lambda)
lasso_fit <- fit(final_lasso, data = train_df)
glmnet_fit <- pull_workflow_fit(lasso_fit)$fit

coef_mat <- coef(glmnet_fit, s = best_lambda$penalty)  # best_lambda is a tibble
coef_df <- as.data.frame(as.matrix(coef_mat))
coef_df$term <- rownames(coef_df)
colnames(coef_df)[1] <- "estimate"

# Select nonzero coefficients (excluding intercept)
selected_vars <- coef_df |> 
  filter(estimate != 0, term != "(Intercept)") |> 
  pull(term)

selected_vars

dropped_vars <- coef_df |>
  filter(estimate == 0, term != "(Intercept)") |>
  pull(term)

dropped_vars
```

## SMOTE

```{r}
# Determining best threshold for classifying true
#cv_preds <- lasso_tuned$.predictions[[1]]

#thresholds <- seq(0, 1, by = 0.01)

#f1_by_thresh <- sapply(thresholds, function(t){
#  cv_preds_thresh <- cv_preds |>
#    mutate(pred_class_thresh = factor(
#      ifelse(.pred_1 >= t, 1, 0), 
#      levels = levels(is_SU)  # match outcome levels
#    ))
#  f_meas(cv_preds_thresh, truth = is_SU, estimate = pred_class_thresh)$.estimate
#})

#best_thresh <- thresholds[which.max(f1_by_thresh)]
#best_thresh

#plot(thresholds, f1_by_thresh, type = "b", xlab = "Threshold", ylab = "F1 Score")
#abline(v = thresholds[which.max(f1_by_thresh)], col = "red", lty = 2)  # best threshold
best_thresh = 0.5
```

```{r}
glm_formula <- as.formula(
    paste("is_SU ~", paste(selected_vars, collapse = " + "))
  )
glm_fit <- glm(
  glm_formula,
  data = bake(prep(su_recipe_lasso), new_data = NULL),
  family = binomial
)
summary(glm_fit)
#exp(coef(glm_fit))
#exp(confint(glm_fit))
```

```{r}
# Evaluate performance on test
test_probs <- predict(lasso_fit, new_data = test_df, type = "prob")
test_results <- test_df |>
  select(is_SU) |>
  bind_cols(test_probs) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1)

# Confusion matrix
conf_mat(test_results, truth = is_SU, estimate = pred_class)

```

### Testing studied_away

```{r}
# Model without studied_away

su_recipe_nostudaway <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nostudaway <- setdiff(selected_vars, "studied_away")
glm_formula_nostudaway <- as.formula(
    paste("is_SU ~", paste(vars_nostudaway, collapse = " + "))
  )
glm_fit_nostudaway <- glm(
  glm_formula_nostudaway,
  data = bake(prep(su_recipe_nostudaway), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nostudaway)
```

### Testing took_summer_courses

```{r}
# Model without took_summer_courses

su_recipe_nosumm <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nosumm <- setdiff(selected_vars, "took_summer_courses")
glm_formula_nosumm <- as.formula(
    paste("is_SU ~", paste(vars_nosumm, collapse = " + "))
  )
glm_fit_nosumm <- glm(
  glm_formula_nosumm,
  data = bake(prep(su_recipe_nosumm), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nosumm)
```

### Testing num_plans

```{r}
# Model without num_plans

su_recipe_noplans <- recipe(is_SU ~ term_units + prev_semGPA +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_noplans <- setdiff(selected_vars, "num_plans")
glm_formula_noplans <- as.formula(
    paste("is_SU ~", paste(vars_noplans, collapse = " + "))
  )
glm_fit_noplans <- glm(
  glm_formula_noplans,
  data = bake(prep(su_recipe_noplans), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_noplans)
```

### Testing max_units

```{r}
# Model without max_units

su_recipe_nomaxu <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nomaxu <- setdiff(selected_vars, "max_units")
glm_formula_nomaxu <- as.formula(
    paste("is_SU ~", paste(vars_nomaxu, collapse = " + "))
  )
glm_fit_nomaxu <- glm(
  glm_formula_nomaxu,
  data = bake(prep(su_recipe_nomaxu), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nomaxu)
```

### Testing term_units vs load_status

```{r}
# Model swapping term_units for load_status

su_recipe_load <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_load <- setdiff(selected_vars, "term_units")
glm_formula_load <- as.formula(
    paste("is_SU ~", paste(vars_load, collapse = " + "))
  )
glm_fit_load <- glm(
  glm_formula_load,
  data = bake(prep(su_recipe_load), new_data = NULL),
  family = binomial
)
```

### AIC and BICs

```{r}
# Put all models in a named list
models <- list(
  Full = glm_fit,
  No_studied_away = glm_fit_nostudaway,
  No_took_summer_courses = glm_fit_nosumm,
  No_num_plans = glm_fit_noplans,
  No_max_units = glm_fit_nomaxu,
  Load_status_instead_of_term_units = glm_fit_load
)

# Compute AIC and BIC for each model
model_table <- tibble(
  Model = names(models),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC)
) |>
  arrange(AIC) |>
  mutate(
    delta_AIC = AIC - min(AIC),
    delta_BIC = BIC - min(BIC)
  )

model_table

```

## Weighted 

```{r}
# Weighted version
# Compute class weights
w_pos <- nrow(train_df) / sum(train_df$is_SU == 1)
w_neg <- nrow(train_df) / sum(train_df$is_SU == 0)

train_df_weighted <- train_df |>
  mutate(weight = ifelse(is_SU == 1, w_pos, w_neg))

su_recipe <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      student_group + timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads + weight,
                    data = train_df_weighted) |>
  update_role(weight, new_role = "case_weight") |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors())

weighted_spec <- logistic_reg(mode = "classification") |>
  set_engine("glm")

weighted_workflow <- workflow() |>
  add_model(weighted_spec) |>
  add_recipe(su_recipe)

# Grouped cross-validation
folds <- group_vfold_cv(train_df_weighted, group = masked_student_ID, v = 5)

# Fit with resampling
weighted_res <- fit_resamples(
  weighted_workflow,
  resamples = folds,
  metrics = metric_set(roc_auc, accuracy),
  control = control_resamples(save_pred = TRUE)
)

# Collect all predictions from CV folds
#cv_preds <- collect_predictions(weighted_res)

# Try multiple thresholds and compute F1
#thresholds <- seq(0.05, 0.95, by = 0.05)
#f1_by_thresh <- sapply(thresholds, function(t) {
#  cv_preds_thresh <- cv_preds |>
#    mutate(pred_class_thresh = factor(ifelse(.pred_1 >= t, 1, 0),
#                                      levels = levels(is_SU)))
#  f_meas(cv_preds_thresh, truth = is_SU, estimate = pred_class_thresh)$.estimate
#})

#best_thresh <- thresholds[which.max(f1_by_thresh)]
#best_thresh
best_thresh = 0.5
#tibble(thresholds, f1 = f1_by_thresh) |>
#  ggplot(aes(thresholds, f1)) +
#  geom_line() + geom_point() +
#  geom_vline(xintercept = best_thresh, linetype = "dashed", color = "red") +
#  labs(title = "F1 vs Threshold (Training CV)", y = "F1 Score")

# Resulting model
final_weighted_fit <- fit(weighted_workflow, data = train_df_weighted)
final_model <- pull_workflow_fit(final_weighted_fit)
summary(final_model$fit)
#exp(coef(final_model$fit))

# Add a dummy weight column to test data
test_df_weighted <- test_df |>
  mutate(weight = 1)  # all equal weights for prediction

# Evaluate performance on test
test_probs <- predict(lasso_fit, new_data = test_df_weighted, type = "prob")
test_results <- test_df_weighted |>
  select(is_SU) |>
  bind_cols(test_probs) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1)

# Confusion matrix
conf_mat(test_results, truth = is_SU, estimate = pred_class)
```

## With Random Effect

```{r}
train_df <- train_df |>
  mutate(final_grade = recode(final_grade,
    "A+" = "A_plus",
    "A"  = "A",
    "A-" = "A_minus",
    "B+" = "B_plus",
    "B"  = "B",
    "B-" = "B_minus",
    "C+" = "C_plus",
    "C"  = "C",
    "C-" = "C_minus",
    "D+" = "D_plus",
    "D"  = "D",
    "D-" = "D_minus",
    "F"  = "F"
  ))

test_df <- test_df |>
  mutate(final_grade = recode(final_grade,
    "A+" = "A_plus",
    "A"  = "A",
    "A-" = "A_minus",
    "B+" = "B_plus",
    "B"  = "B",
    "B-" = "B_minus",
    "C+" = "C_plus",
    "C"  = "C",
    "C-" = "C_minus",
    "D+" = "D_plus",
    "D"  = "D",
    "D-" = "D_minus",
    "F"  = "F"
  ))

# Train set
train_df |> count(is_SU)

# Test set
test_df |> count(is_SU)

#train_idx <- createDataPartition(train_df$is_SU, p = 1000 / nrow(train_df), list = FALSE)
#train_df_small <- train_df[train_idx, ]

#test_idx <- createDataPartition(test_df_weighted$is_SU, p = 1000 / nrow(test_df), list = FALSE)
#test_df_small <- test_df[test_idx, ]

train_df_small <- train_df |>
  group_by(is_SU) |>
  slice_sample(n = 5000) |>  # 3000 rows per class
  ungroup()

test_df_small <- test_df |>
  group_by(is_SU) |>
  slice_sample(n = 1000) |>  # 1000 rows per class
  ungroup()

glmer_recipe <- recipe(is_SU ~ ., data = train_df_small) |>
  update_role(masked_student_ID, new_role = "id") |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors(), -all_outcomes(), -masked_student_ID) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors())  # remove zero-variance predictors

# 2. Prep and bake
train_baked <- prep(glmer_recipe) |> bake(new_data = NULL)

# 3. Keep only LASSO-selected columns + masked_student_ID + outcome
glmer_data <- train_baked |>
  select(all_of(selected_vars), masked_student_ID, is_SU)

# 4. Build formula dynamically
glm_formula <- as.formula(
  paste0("is_SU ~ ", paste(selected_vars, collapse = " + "), " + (1 | masked_student_ID)")
)

# 5. Fit glmer
glmer_fit <- glmer(
  glm_formula,
  data = glmer_data,
  family = binomial(link = "logit"),
  control = glmerControl(optimizer = "bobyqa", calc.derivs = FALSE)
)
summary(glmer_fit)

# Evaluate performance on test
best_thesh = 0.5
test_probs <- predict(lasso_fit, new_data = test_df_small, type = "prob")
test_results <- test_df_small |>
  select(is_SU) |>
  bind_cols(test_probs) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1)

# Confusion matrix
conf_mat(test_results, truth = is_SU, estimate = pred_class)

```

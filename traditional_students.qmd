---
title: "traditional_students"
format: pdf
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: load-libraries
#| message: false
library(tidyverse)
library(viridis)
library(lme4)
library(glmnet)
library(pROC)
library(tidymodels)
library(rsample)
library(themis)
library(caret)
library(DHARMa)
```

```{r}
#| label: load-data
df <- readRDS("data/model_df.rds")
df <- df |>
  mutate(
    studied_away = if_else(is.na(studied_away), FALSE, studied_away),
    prev_semGPA  = if_else(is.na(prev_semGPA), 4, prev_semGPA),
  )
```

```{r}
#| label: class-imbalance

# Imbalance of S/U vs A-F
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))

# Imbalance by student group
df |>
  group_by(student_group) |>
  summarise(
    n_total   = n(),
    n_true    = sum(is_SU == "1", na.rm = TRUE),
    n_false   = sum(is_SU == "0", na.rm = TRUE),
    prop_true = mean(is_SU == "1", na.rm = TRUE)
  ) |>
  arrange(desc(prop_true))

# Chose to drop data prior to Fall 2015
df <- df |>
  mutate(is_SU = factor(is_SU, levels = c(0, 1))) |>
  filter(term_enrolled > "Summer 2 2015")

# Slightly improved imbalance of S/U A-F
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))

```

```{r}
#| label: feature-imbalance

# Most courses are 1.0 credits
df |>
  count(max_units) |>
  mutate(prop = n / sum(n))

# A lot of uncommon term_units
df |>
  count(term_units) |>
  mutate(prop = n / sum(n)) |>
  print(n = Inf)

df <- df |>
  mutate(
    load_status = case_when(
      term_units < 4    ~ "underload",
      term_units == 4   ~ "normal",
      term_units > 4    ~ "overload",
      TRUE              ~ NA_character_   # handles any missing cases
    )
  )

# Relatively balanced when using categories instead of units
df |>
  count(load_status) |>
  mutate(prop = n / sum(n))

df <- df |>
  select(-c("term_enrolled", "course_ID", "estimated", "total_overloads", "student_group", "year_enrolled"))
```

```{r}
#| label: train-test-split
set.seed(123)

split <- group_initial_split(df, group = masked_student_ID, prop = 0.8)
train_df <- training(split)
test_df  <- testing(split)

# want to test on students (no data leakage about students)
```

```{r}
#| label: preprocess
su_recipe_lasso <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

```

## LASSO 

```{r}
#| message: false
lasso_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),  # lambda
  mixture = 1        # pure LASSO
) |>
  set_engine("glmnet")

lasso_workflow <- workflow() |>
  add_model(lasso_spec) |>
  add_recipe(su_recipe_lasso)

folds <- group_vfold_cv(train_df, group = masked_student_ID, v = 5)
lambda_grid <- grid_regular(penalty(range = c(-4, 1)), levels = 20)
lasso_tuned <- tune_grid(
  lasso_workflow,
  resamples = folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, accuracy),
  control = control_grid(save_pred = TRUE, verbose = TRUE)
)

best_lambda <- select_best(lasso_tuned, metric = "roc_auc")

final_lasso <- finalize_workflow(lasso_workflow, best_lambda)
lasso_fit <- fit(final_lasso, data = train_df)
glmnet_fit <- extract_fit_parsnip(lasso_fit)$fit

coef_mat <- coef(glmnet_fit, s = best_lambda$penalty)  # best_lambda is a tibble
coef_df <- as.data.frame(as.matrix(coef_mat))
coef_df$term <- rownames(coef_df)
colnames(coef_df)[1] <- "estimate"

# Select nonzero coefficients (excluding intercept)
selected_vars <- coef_df |> 
  filter(estimate != 0, term != "(Intercept)") |> 
  pull(term)

selected_vars

dropped_vars <- coef_df |>
  filter(estimate == 0, term != "(Intercept)") |>
  pull(term)

dropped_vars

# use default threshold of 0.5
best_thresh = 0.5
```

## SMOTE

```{r}
glm_formula <- as.formula(
    paste("is_SU ~", paste(selected_vars, collapse = " + "))
  )
glm_fit <- glm(
  glm_formula,
  data = bake(prep(su_recipe_lasso), new_data = NULL),
  family = binomial
)
summary(glm_fit)
#exp(coef(glm_fit))
#exp(confint(glm_fit))
```

```{r}
# Evaluate performance on test
test_probs <- predict(lasso_fit, new_data = test_df, type = "prob")
test_results <- test_df |>
  select(is_SU) |>
  bind_cols(test_probs) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1, estimator = "binary", event_level = "second")

# Confusion matrix
conf_mat(test_results, truth = is_SU, estimate = pred_class)

```

### Testing studied_away

```{r}
# Model without studied_away
su_recipe_nostudaway <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nostudaway <- setdiff(selected_vars, "studied_away")
glm_formula_nostudaway <- as.formula(
    paste("is_SU ~", paste(vars_nostudaway, collapse = " + "))
  )
glm_fit_nostudaway <- glm(
  glm_formula_nostudaway,
  data = bake(prep(su_recipe_nostudaway), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nostudaway)
```

### Testing took_summer_courses

```{r}
# Model without took_summer_courses
su_recipe_nosumm <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      studied_away +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nosumm <- setdiff(selected_vars, "took_summer_courses")
glm_formula_nosumm <- as.formula(
    paste("is_SU ~", paste(vars_nosumm, collapse = " + "))
  )
glm_fit_nosumm <- glm(
  glm_formula_nosumm,
  data = bake(prep(su_recipe_nosumm), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nosumm)
```

### Testing num_plans

```{r}
# Model without num_plans
su_recipe_noplans <- recipe(is_SU ~ load_status + prev_semGPA +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_noplans <- setdiff(selected_vars, "num_plans")
glm_formula_noplans <- as.formula(
    paste("is_SU ~", paste(vars_noplans, collapse = " + "))
  )
glm_fit_noplans <- glm(
  glm_formula_noplans,
  data = bake(prep(su_recipe_noplans), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_noplans)
```

### Testing max_units

```{r}
# Model without max_units
su_recipe_nomaxu <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nomaxu <- setdiff(selected_vars, "max_units")
glm_formula_nomaxu <- as.formula(
    paste("is_SU ~", paste(vars_nomaxu, collapse = " + "))
  )
glm_fit_nomaxu <- glm(
  glm_formula_nomaxu,
  data = bake(prep(su_recipe_nomaxu), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nomaxu)
```

### Testing term_units vs load_status

```{r}
# Model swapping load_status for term_units
su_recipe_units <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_units <- selected_vars[!str_detect(selected_vars, "load_status")]
glm_formula_units <- as.formula(
    paste("is_SU ~", paste(vars_units, collapse = " + "))
  )
glm_fit_units <- glm(
  glm_formula_units,
  data = bake(prep(su_recipe_units), new_data = NULL),
  family = binomial
)
```

### AIC and BICs

```{r}
# Put all models in a named list
models <- list(
  Full = glm_fit,
  No_studied_away = glm_fit_nostudaway,
  No_took_summer_courses = glm_fit_nosumm,
  No_num_plans = glm_fit_noplans,
  No_max_units = glm_fit_nomaxu,
  Load_status_instead_of_term_units = glm_fit_units
)

# Compute AIC and BIC for each model
model_table <- tibble(
  Model = names(models),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC)
) |>
  arrange(AIC) |>
  mutate(
    delta_AIC = AIC - min(AIC),
    delta_BIC = BIC - min(BIC)
  )

model_table

```

## Weighted 

```{r}
# Weighted version
# Compute class weights
w_pos <- nrow(train_df) / sum(train_df$is_SU == 1)
w_neg <- nrow(train_df) / sum(train_df$is_SU == 0)

train_df_weighted <- train_df |>
  mutate(weight = ifelse(is_SU == 1, w_pos, w_neg))

su_recipe <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      max_units + num_students +
                      num_overloads + weight,
                    data = train_df_weighted) |>
  update_role(weight, new_role = "case_weight") |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors())

weighted_spec <- logistic_reg(mode = "classification") |>
  set_engine("glm")

weighted_workflow <- workflow() |>
  add_model(weighted_spec) |>
  add_recipe(su_recipe)

# Grouped cross-validation
folds <- group_vfold_cv(train_df_weighted, group = masked_student_ID, v = 5)

# Fit with resampling
weighted_res <- fit_resamples(
  weighted_workflow,
  resamples = folds,
  metrics = metric_set(roc_auc, accuracy),
  control = control_resamples(save_pred = TRUE)
)

# Resulting model
final_weighted_fit <- fit(weighted_workflow, data = train_df_weighted)
final_model <- extract_fit_parsnip(final_weighted_fit)
summary(final_model$fit)
#exp(coef(final_model$fit))

# Add a dummy weight column to test data
test_df_weighted <- test_df |>
  mutate(weight = 1)  # all equal weights for prediction

# Evaluate performance on test
test_probs <- predict(lasso_fit, new_data = test_df_weighted, type = "prob")
test_results <- test_df_weighted |>
  select(is_SU) |>
  bind_cols(test_probs) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1, estimator = "binary", event_level = "second")

# Confusion matrix
conf_mat(test_results, truth = is_SU, estimate = pred_class)
```

## With Random Effect

```{r}
train_df <- train_df |>
  mutate(final_grade = recode(final_grade,
    "A+" = "A_plus",
    "A"  = "A",
    "A-" = "A_minus",
    "B+" = "B_plus",
    "B"  = "B",
    "B-" = "B_minus",
    "C+" = "C_plus",
    "C"  = "C",
    "C-" = "C_minus",
    "D+" = "D_plus",
    "D"  = "D",
    "D-" = "D_minus",
    "F"  = "F"
  ))

test_df <- test_df |>
  mutate(final_grade = recode(final_grade,
    "A+" = "A_plus",
    "A"  = "A",
    "A-" = "A_minus",
    "B+" = "B_plus",
    "B"  = "B",
    "B-" = "B_minus",
    "C+" = "C_plus",
    "C"  = "C",
    "C-" = "C_minus",
    "D+" = "D_plus",
    "D"  = "D",
    "D-" = "D_minus",
    "F"  = "F"
  ))

# Train set
train_df |> count(is_SU)

# Test set
test_df |> count(is_SU)

train_df_small <- train_df |>
  slice_sample(n = 20000) |> 
  ungroup()

test_df_small <- test_df |>
  slice_sample(n = 4000) |>
  ungroup()

glmer_recipe <- recipe(is_SU ~ ., data = train_df_small) |>
  update_role(masked_student_ID, new_role = "id") |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_mutate(across(where(is.logical), as.factor)) |>  # convert logicals to factors vs glm wanted binary(?)
  step_dummy(all_nominal_predictors(), -all_outcomes(), -masked_student_ID) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors()) |>  # remove zero-variance predictors
  step_smote(is_SU)

# 2. Prep and bake
train_baked <- prep(glmer_recipe) |> bake(new_data = NULL)

# 3. Keep only LASSO-selected columns + masked_student_ID + outcome
glmer_data <- train_baked |>
  select(all_of(selected_vars), masked_student_ID, is_SU)

# 4. Build formula dynamically
glm_formula <- as.formula(
  paste0("is_SU ~ ", paste(selected_vars, collapse = " + "), " + (1 | masked_student_ID)")
)

# 5. Fit glmer
glmer_fit <- glmer(
  glm_formula,
  data = glmer_data,
  family = binomial(link = "logit"),
  control = glmerControl(optimizer = "bobyqa", calc.derivs = FALSE)
)
summary(glmer_fit)

# Evaluate performance on test
test_probs <- predict(lasso_fit, new_data = test_df_small, type = "prob")
test_results <- test_df_small |>
  select(is_SU) |>
  bind_cols(test_probs) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

```

```{r}
# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1, estimator = "binary", event_level = "second")

# PR-AUC
pr_auc_val <- pr_auc(
  test_results, truth = is_SU, .pred_1,
  event_level = "second"
)
pr_auc_val

# PR curve plot
pr_curve_df <- pr_curve(test_results, truth = is_SU, .pred_1, event_level = "second")
ggplot(pr_curve_df, aes(x = recall, y = precision)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "Precision-Recall Curve")

# Confusion matrix
cm <- conf_mat(test_results, truth = is_SU, estimate = pred_class)
cm

# Extract Precision, Recall, F1
cm_metrics <- cm |> 
  summary() |> 
  filter(.metric %in% c("precision", "recall", "f_meas"))
cm_metrics

# ---- Calibration plot ----
# Bin predicted probabilities into deciles
calibration_df <- test_results |>
  mutate(prob_bin = ntile(.pred_1, 10)) |>
  group_by(prob_bin) |>
  summarise(
    mean_pred = mean(.pred_1),
    obs_rate = mean(as.numeric(is_SU)),
    .groups = "drop"
  )

ggplot(calibration_df, aes(x = mean_pred, y = obs_rate)) +
  geom_point(size = 3, color = "blue") +
  geom_line(color = "blue") +
  # Add the "perfect calibration" line
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) +
  labs(
    x = "Mean Predicted Probability",
    y = "Observed Event Rate",
    title = "Calibration Plot"
  )
```

```{r}

# ---- 1. Extract fitted values and Pearson residuals ----
fitted_vals <- fitted(glmer_fit)               # predicted probabilities
pearson_res <- residuals(glmer_fit, type = "pearson")

# ---- 2. Binned residuals: bin predicted probabilities into deciles ----
binned_df <- tibble(
  fitted = fitted_vals,
  resid = pearson_res,
  obs = as.numeric(glmer_fit@resp$y)
) %>%
  mutate(bin = ntile(fitted, 10)) %>%
  group_by(bin) %>%
  summarise(
    mean_fitted = mean(fitted),
    mean_resid = mean(resid),
    obs_rate = mean(obs),
    .groups = "drop"
  )

# ---- 3. Plot: mean residual vs mean predicted probability per bin ----
p1 <- ggplot(binned_df, aes(x = mean_fitted, y = mean_resid)) +
  geom_point(size = 3, color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "Mean Predicted Probability (bin)",
    y = "Mean Pearson Residual",
    title = "Binned Residuals vs Predicted Probability"
  )

# ---- 4. Plot: predicted vs observed probability per bin ----
p2 <- ggplot(binned_df, aes(x = mean_fitted, y = obs_rate)) +
  geom_point(size = 3, color = "darkgreen") +
  geom_line(color = "darkgreen") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(
    x = "Mean Predicted Probability (bin)",
    y = "Observed Event Rate",
    title = "Predicted vs Observed Probability"
  )

# ---- 5. Random intercepts Q-Q plot ----
ranef_df <- ranef(glmer_fit)$masked_student_ID %>%
  rownames_to_column("student_ID") %>%
  rename(intercept = `(Intercept)`)

p3 <- ggplot(ranef_df, aes(sample = intercept)) +
  stat_qq() +
  stat_qq_line(color = "blue") +
  labs(title = "Q-Q Plot of Random Intercepts (masked_student_ID)")

# ---- 6. Arrange plots together ----
library(gridExtra)
grid.arrange(p1, p2, p3, nrow = 2)

```

```{r}
# Simulate new residuals from the fitted model
# n = 250 is a good default number of simulations
simulationOutput <- simulateResiduals(fittedModel = glmer_fit, n = 250)

# 8a. Main Diagnostic Plot
print("--- Overdispersion Check (DHARMa Method) ---")
print("Generating DHARMa plot...")

# This will create a plot in your plot pane.
# LEFT: A Q-Q plot. The red line should be straight and on the diagonal.
#       The p-value is for the KS-test (you want p > 0.05).
# RIGHT: Residuals vs. Predicted. You want no pattern.
# DHARMa also runs tests for dispersion and outliers.
plot(simulationOutput)

# 8b. Explicit Test for Overdispersion
# This test is more reliable than the quick method.
print("Running DHARMa overdispersion test:")
testOverdispersion(simulationOutput)

# 8c. You can also test for zero-inflation (often related to overdispersion)
print("Running DHARMa zero-inflation test:")
testZeroInflation(simulationOutput)
```


---
title: "traditional_students"
format: pdf
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: load-libraries
#| message: false
library(tidyverse)
library(viridis)
library(lme4)
library(glmnet)
library(pROC)
library(tidymodels)
library(rsample)
library(themis)
library(caret)
library(performance)
```

```{r}
#| label: load-data
df <- readRDS("data/model_df.rds")
df <- df |>
  mutate(
    studied_away = if_else(is.na(studied_away), FALSE, studied_away),
    prev_semGPA  = if_else(is.na(prev_semGPA), 4, prev_semGPA),
  )
```

```{r}
#| label: class-imbalance

# Imbalance of S/U vs A-F
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))

# Imbalance by student group
df |>
  group_by(student_group) |>
  summarise(
    n_total   = n(),
    n_true    = sum(is_SU == "1", na.rm = TRUE),
    n_false   = sum(is_SU == "0", na.rm = TRUE),
    prop_true = mean(is_SU == "1", na.rm = TRUE)
  ) |>
  arrange(desc(prop_true))

# Chose to drop data prior to Fall 2015
df <- df |>
  mutate(is_SU = factor(is_SU, levels = c(0, 1))) |>
  filter(term_enrolled > "Summer 2 2015")

# Slightly improved imbalance of S/U A-F
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))

```

```{r}
#| label: feature-imbalance

# Most courses are 1.0 credits
df |>
  count(max_units) |>
  mutate(prop = n / sum(n))
# Can only S/U 1.0 credit courses(?)
df <- df |>
  filter(max_units == 1.0)

# A lot of uncommon term_units
df |>
  count(term_units) |>
  mutate(prop = n / sum(n)) |>
  print(n = Inf)

df <- df |>
  mutate(
    load_status = case_when(
      term_units < 4    ~ "underload",
      term_units == 4   ~ "normal",
      term_units > 4    ~ "overload",
      TRUE              ~ NA_character_   # handles any missing cases
    )
  )

# Relatively balanced when using categories instead of units
df |>
  count(load_status) |>
  mutate(prop = n / sum(n))

df <- df |>
  select(-c("term_enrolled", "course_ID", "estimated", "total_overloads", "student_group", "year_enrolled"))

# Final class imbalance
df |>
  count(is_SU) |>
  mutate(prop = n / sum(n))
```

## Train Test Split

```{r}
#| label: train-test-split
set.seed(123)

split <- group_initial_split(df, group = masked_student_ID, prop = 0.8)
train_df <- training(split)
test_df  <- testing(split)

# want to test on students (no data leakage about students)
```

## Hyperparameter (lambda for LASSO, over ratio for SMOTE, decision threshold) Tuning

```{r}
#| label: preprocess
#| eval: false
su_recipe_lasso <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU, over_ratio = tune()) # correct imbalance
```

```{r}
#| message: false
#| eval: false
lasso_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),  # lambda
  mixture = tune()
) |>
  set_engine("glmnet")

lasso_workflow <- workflow() |>
  add_model(lasso_spec) |>
  add_recipe(su_recipe_lasso)

# tune and estimate simultaneously (ok since exploratory)
folds <- group_vfold_cv(train_df, group = masked_student_ID, v = 5)
grid <- grid_regular(
  over_ratio(range = c(0.2, 1)),
  penalty(range = c(-4, 1)),
  mixture(range = c(0, 1)),
  levels = 5
)
lasso_tuned <- tune_grid(
  lasso_workflow,
  resamples = folds,
  grid = grid,
  metrics = metric_set(roc_auc, accuracy, f_meas),
  control = control_grid(save_pred = TRUE, verbose = TRUE)
)

best_hyperparam <- select_best(lasso_tuned, metric = "roc_auc")
best_hyperparam
saveRDS(best_hyperparam, "data/best_hyperparam.rds")

final_lasso <- finalize_workflow(lasso_workflow, best_combo)
final_fit <- fit(final_lasso, data = train_df)

test_results <- predict(final_fit, new_data = test_df, type = "prob") |>
  bind_cols(test_df) |>
  roc_auc(truth = is_SU, .pred_1)
glmnet_fit <- extract_fit_parsnip(final_fit)$fit

coef_mat <- coef(glmnet_fit, s = best_combo$penalty)
coef_df <- as.data.frame(as.matrix(coef_mat))
coef_df$term <- rownames(coef_df)
colnames(coef_df)[1] <- "estimate"

# Select nonzero coefficients (excluding intercept)
selected_vars <- coef_df |> 
  filter(estimate != 0, term != "(Intercept)") |> 
  pull(term)

selected_vars
saveRDS(selected_vars, "data/selected_vars.rds")

dropped_vars <- coef_df |>
  filter(estimate == 0, term != "(Intercept)") |>
  pull(term)

dropped_vars
saveRDS(dropped_vars, "data/dropped_vars.rds")
```

```{r}
#| eval: false
# Suppose you have CV predictions from tune_grid()
preds <- lasso_tuned$.predictions[[1]]

# Evaluate a range of thresholds
thresholds <- seq(0.01, 0.5, by = 0.01)  # explore lower since positive class is rare

results <- map_dfr(thresholds, ~{
  preds <- preds |>
    mutate(.pred_class = factor(ifelse(.pred_1 >= .x, 1, 0), levels = c(0,1)))
  
  metric <- f_meas(preds, truth = is_SU, estimate = .pred_class)
  tibble(threshold = .x, F1 = metric$.estimate)
})

# Find best threshold
best_thresh <- results |> filter(F1 == max(F1)) |> pull(threshold)
best_thresh
saveRDS(best_thresh, "data/best_thresh.rds")
```

```{r}
best_hyperparam <- readRDS("data/best_hyperparam.rds")
best_thresh <- readRDS("data/best_thresh.rds")
selected_vars <- readRDS("data/selected_vars.rds")

su_recipe_fixed <- recipe(is_SU ~ ., data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors()) |>
  step_smote(is_SU, over_ratio = best_hyperparam$over_ratio)  # plug in tuned value

glm_formula <- as.formula(
    paste("is_SU ~", paste(selected_vars, collapse = " + "))
  )
glm_fit <- glm(
  glm_formula,
  data = bake(prep(su_recipe_fixed), new_data = NULL),
  family = binomial
)
summary(glm_fit)
#exp(coef(glm_fit))
#exp(confint(glm_fit))
```

```{r}
# Evaluate performance on test
test_results <- augment(lasso_fit, new_data = test_df) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1, estimator = "binary", event_level = "second")

# Confusion matrix
conf_mat(test_results, truth = is_SU, estimate = pred_class)

```

## Explicit Variable Checks

### Testing studied_away

```{r}
# Model without studied_away
su_recipe_nostudaway <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nostudaway <- setdiff(selected_vars, "studied_away")
glm_formula_nostudaway <- as.formula(
    paste("is_SU ~", paste(vars_nostudaway, collapse = " + "))
  )
glm_fit_nostudaway <- glm(
  glm_formula_nostudaway,
  data = bake(prep(su_recipe_nostudaway), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nostudaway)
```

### Testing took_summer_courses

```{r}
# Model without took_summer_courses
su_recipe_nosumm <- recipe(is_SU ~ load_status + prev_semGPA + num_plans +
                      studied_away +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_nosumm <- setdiff(selected_vars, "took_summer_courses")
glm_formula_nosumm <- as.formula(
    paste("is_SU ~", paste(vars_nosumm, collapse = " + "))
  )
glm_fit_nosumm <- glm(
  glm_formula_nosumm,
  data = bake(prep(su_recipe_nosumm), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_nosumm)
```

### Testing num_plans

```{r}
# Model without num_plans
su_recipe_noplans <- recipe(is_SU ~ load_status + prev_semGPA +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_noplans <- setdiff(selected_vars, "num_plans")
glm_formula_noplans <- as.formula(
    paste("is_SU ~", paste(vars_noplans, collapse = " + "))
  )
glm_fit_noplans <- glm(
  glm_formula_noplans,
  data = bake(prep(su_recipe_noplans), new_data = NULL),
  family = binomial
)

# ANOVA
anova(glm_fit, glm_fit_noplans)
```

### Testing term_units vs load_status

```{r}
# Model swapping load_status for term_units
su_recipe_units <- recipe(is_SU ~ term_units + prev_semGPA + num_plans +
                      studied_away + took_summer_courses +
                      is_art_humanity + is_social_sci + is_natural_sci +
                      timeperiod + academic_level_bot +
                      catalog_level + division +
                      num_students +
                      num_overloads,
                    data = train_df) |>
  step_impute_median(all_numeric_predictors()) |>  # median imputation for numeric
  step_impute_mode(all_nominal_predictors()) |>    # mode imputation for factors
  step_dummy(all_nominal_predictors()) |>          # one-hot encode factors
  step_normalize(all_numeric_predictors()) |>      # center + scale automatically
  step_zv(all_predictors()) |>
  step_smote(is_SU) # only 2% of data is S/U, correct imbalance

vars_units <- selected_vars[!str_detect(selected_vars, "load_status")]
glm_formula_units <- as.formula(
    paste("is_SU ~", paste(vars_units, collapse = " + "))
  )
glm_fit_units <- glm(
  glm_formula_units,
  data = bake(prep(su_recipe_units), new_data = NULL),
  family = binomial
)
```

### AIC and BICs

```{r}
# Put all models in a named list
models <- list(
  Full = glm_fit,
  No_studied_away = glm_fit_nostudaway,
  No_took_summer_courses = glm_fit_nosumm,
  No_num_plans = glm_fit_noplans,
  Term_units_instead_of_load_status = glm_fit_units
)

# Compute AIC and BIC for each model
model_table <- tibble(
  Model = names(models),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC)
) |>
  arrange(AIC) |>
  mutate(
    delta_AIC = AIC - min(AIC),
    delta_BIC = BIC - min(BIC)
  )

model_table

```

## With Random Effect

```{r}
train_df <- train_df |>
  mutate(final_grade = recode(final_grade,
    "A+" = "A_plus",
    "A"  = "A",
    "A-" = "A_minus",
    "B+" = "B_plus",
    "B"  = "B",
    "B-" = "B_minus",
    "C+" = "C_plus",
    "C"  = "C",
    "C-" = "C_minus",
    "D+" = "D_plus",
    "D"  = "D",
    "D-" = "D_minus",
    "F"  = "F"
  ))

test_df <- test_df |>
  mutate(final_grade = recode(final_grade,
    "A+" = "A_plus",
    "A"  = "A",
    "A-" = "A_minus",
    "B+" = "B_plus",
    "B"  = "B",
    "B-" = "B_minus",
    "C+" = "C_plus",
    "C"  = "C",
    "C-" = "C_minus",
    "D+" = "D_plus",
    "D"  = "D",
    "D-" = "D_minus",
    "F"  = "F"
  ))

# Train set
train_df |> count(is_SU)

# Test set
test_df |> count(is_SU)

train_df_small <- train_df |>
  slice_sample(n = 20000) |> 
  ungroup()

test_df_small <- test_df |>
  slice_sample(n = 4000) |>
  ungroup()

glmer_recipe <- recipe(is_SU ~ ., data = train_df_small) |>
  update_role(masked_student_ID, new_role = "id") |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_mutate(across(where(is.logical), as.factor)) |>  # convert logicals to factors vs glm wanted binary(?)
  step_dummy(all_nominal_predictors(), -all_outcomes(), -masked_student_ID) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors()) |>  # remove zero-variance predictors
  step_smote(is_SU)

# 2. Prep and bake
train_baked <- prep(glmer_recipe) |> bake(new_data = NULL)

# 3. Keep only LASSO-selected columns + masked_student_ID + outcome
glmer_data <- train_baked |>
  select(all_of(selected_vars), masked_student_ID, is_SU)

# 4. Build formula dynamically
glm_formula <- as.formula(
  paste0("is_SU ~ ", paste(selected_vars, collapse = " + "), " + (1 | masked_student_ID)")
)

# 5. Fit glmer
glmer_fit <- glmer(
  glm_formula,
  data = glmer_data,
  family = binomial(link = "logit"),
  control = glmerControl(optimizer = "bobyqa", calc.derivs = FALSE)
)
summary(glmer_fit)

# Evaluate performance on test
test_results <- augment(lasso_fit, new_data = test_df_small) |>
  mutate(
    pred_class = ifelse(.pred_1 >= best_thresh, 1, 0),
    pred_class = factor(pred_class, levels = c(0, 1))
  )

```

```{r}
# AIC
AIC(glmer_fit)

# BIC
BIC(glmer_fit)

# ICC
vc <- as.data.frame(VarCorr(glmer_fit))
vc
var_student <- vc$vcov[vc$grp == "masked_student_ID"]
var_resid <- pi^2 / 3
var_student / (var_student + var_resid)
icc(glmer_fit)

# RMSE
test_results_num <- test_results |>
  mutate(is_SU_num = as.numeric(as.character(is_SU)))
rmse_tbl <- rmse(test_results_num, truth = is_SU_num, estimate = .pred_1)
if (is.data.frame(rmse_tbl)) {
  model_rmse <- rmse_tbl$.estimate[1]
} else {
  model_rmse <- as.numeric(rmse_tbl)
}
mean_prob <- mean(test_results_num$is_SU_num)
null_rmse <- sqrt(mean((test_results_num$is_SU_num - mean_prob)^2))
tibble(
  Model = c("Lasso Model", "Null Model"),
  RMSE = c(model_rmse, null_rmse)
)

# ROC / AUC
roc_auc(test_results, truth = is_SU, .pred_1, estimator = "binary", event_level = "second")

# Plot ROC
roc_df <- roc_curve(test_results, truth = is_SU, .pred_1, event_level = "second")
ggplot(roc_df, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "blue", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") + # diagonal line
  labs(
    title = "ROC Curve",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()

# PR-AUC
pr_auc_val <- pr_auc(
  test_results, truth = is_SU, .pred_1,
  event_level = "second"
)
pr_auc_val

# Plot PR
pr_df <- pr_curve(test_results, truth = is_SU, .pred_1, event_level = "second")
ggplot(pr_df, aes(x = recall, y = precision)) +
  geom_line(color = "darkgreen", size = 1.2) +
  labs(
    title = "Precision-Recall Curve",
    x = "Recall (Sensitivity)",
    y = "Precision (PPV)"
  ) +
  theme_minimal()

# Confusion matrix
cm <- conf_mat(test_results, truth = is_SU, estimate = pred_class)
cm

# Extract Precision, Recall, Specificity F1
cm_metrics <- cm |> 
  summary() |>
  filter(.metric %in% c("precision", "recall", "f_meas", "spec")) |>
  mutate(
    Metric = recode(.metric,
      "precision" = "Precision/Positive Predictive Value",
      "recall"    = "Recall/Sensitivity (True Positive Rate)",
      "spec"      = "Specificity (True Negative Rate)",
      "f_meas"    = "F1 Score",
    )
  )
cm_metrics

# ---- Calibration plot ----
# Bin predicted probabilities into deciles
calibration_df <- test_results |>
  mutate(prob_bin = ntile(.pred_1, 10)) |>
  group_by(prob_bin) |>
  summarise(
    mean_pred = mean(.pred_1),
    obs_rate = mean(as.numeric(as.character(is_SU))),
    .groups = "drop"
  )

ggplot(calibration_df, aes(x = mean_pred, y = obs_rate)) +
  geom_point(size = 3, color = "blue") +
  geom_line(color = "blue") +
  # Add the "perfect calibration" line
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "red",
    linetype = "dashed",
    linewidth = 1
  ) +
  labs(
    x = "Mean Predicted Probability",
    y = "Observed Event Rate",
    title = "Calibration Plot"
  )
```

```{r}
check_overdispersion(glmer_fit)

plot(residuals(glmer_fit) ~ factor(group_variable))

pear <- residuals(glmer_fit, type = "pearson")
df$pear <- pear
student_resid <- df %>%
  group_by(masked_student_ID) %>%
  summarize(n = n(), mean_pear = mean(pear), sd_pear = sd(pear), mean_obs = mean(is_SU))

hist(student_resid$mean_pear, main="Mean Pearson residual per student")
plot(student_resid$n, student_resid$sd_pear,
     xlab="Observations per student", ylab="SD of Pearson residuals per student")

```
